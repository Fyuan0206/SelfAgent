"""
é«˜çº§æƒ…ç»ªç”»åƒç³»ç»Ÿ - æ·±åº¦ç‰ˆ
æä¾›å¤šç»´åº¦ã€é¢„æµ‹æ€§çš„ç”¨æˆ·æƒ…ç»ªåˆ†æ
"""

import json
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict, field
from datetime import datetime, timedelta
from pathlib import Path
from loguru import logger
from collections import defaultdict, Counter
from scipy import stats
from scipy.signal import find_peaks
import hashlib


@dataclass
class EmotionSnapshot:
    """æƒ…ç»ªå¿«ç…§"""
    timestamp: float
    date: str
    emotions: Dict[str, float]
    arousal: float
    route_level: str
    risk_level: str
    context: str
    multimodal_vector: List[float]

    # æ‰©å±•å­—æ®µ
    input_type: str = "unknown"  # text/audio/image
    duration: Optional[float] = None  # äº’åŠ¨æ—¶é•¿
    time_of_day: str = ""  # morning/afternoon/evening/night
    day_of_week: str = ""  # Monday/Tuesday...
    is_weekend: bool = False


@dataclass
class EmotionTrend:
    """æƒ…ç»ªè¶‹åŠ¿"""
    direction: str  # rising/falling/stable/volatile
    slope: float  # å˜åŒ–æ–œç‡
    confidence: float  # ç½®ä¿¡åº¦
    timespan_days: float  # åˆ†ææ—¶é—´è·¨åº¦
    change_percentage: Dict[str, float]  # å„æƒ…ç»ªå˜åŒ–ç™¾åˆ†æ¯”


@dataclass
class EmotionCycle:
    """æƒ…ç»ªå‘¨æœŸ"""
    cycle_type: str  # daily/weekly/monthly
    peak_times: List[str]  # é«˜å³°æ—¶æ®µ
    low_times: List[str]  # ä½è°·æ—¶æ®µ
    strength: float  # å‘¨æœŸå¼ºåº¦
    pattern: Dict[str, List[float]]  # å„æƒ…ç»ªçš„å‘¨æœŸæ¨¡å¼


@dataclass
class EmotionCluster:
    """æƒ…ç»ªèšç±»æ¨¡å¼"""
    cluster_name: str  # èšç±»åç§°
    emotions: Dict[str, float]  # å…¸å‹æƒ…ç»ªç»„åˆ
    frequency: int  # å‡ºç°é¢‘ç‡
    triggers: List[str]  # è§¦å‘å› ç´ 
    outcomes: List[str]  # å¸¸è§ç»“æœ


@dataclass
class PersonalityProfile:
    """æ€§æ ¼ç”»åƒ"""
    # å¤§äº”äººæ ¼
    openness: float = 0.5
    conscientiousness: float = 0.5
    extraversion: float = 0.5
    agreeableness: float = 0.5
    neuroticism: float = 0.5

    # å†³ç­–é£æ ¼
    decision_style: str = "analytical"  # analytical/intuitive/dependent/avoidant
    risk_tolerance: float = 0.5
    coping_style: str = "problem_focused"  # problem_focused/emotion_focused/avoidant

    # ç¤¾äº¤ç‰¹å¾
    social_orientation: str = "ambivert"  # introvert/ambivert/extravert
    expressiveness: float = 0.5
    support_seeking: float = 0.5

    # å‹åŠ›ååº”
    stress_response: str = "active"  # active/passive/withdrawn
    resilience_score: float = 0.5

    # æƒ…ç»ªç‰¹å¾
    emotional_awareness: float = 0.5
    emotion_regulation: float = 0.5
    impulse_control: float = 0.5


@dataclass
class RiskPrediction:
    """é£é™©é¢„æµ‹"""
    next_crisis_probability: float  # ä¸‹æ¬¡å±æœºæ¦‚ç‡
    high_risk_time_windows: List[str]  # é«˜é£é™©æ—¶æ®µ
    early_warning_signals: List[str]  # æ—©æœŸé¢„è­¦ä¿¡å·
    protective_factors: List[str]  # ä¿æŠ¤å› ç´ 
    recommended_monitoring: List[str]  # å»ºè®®ç›‘æ§æŒ‡æ ‡


@dataclass
class AdvancedEmotionProfile:
    """é«˜çº§æƒ…ç»ªç”»åƒ"""
    # åŸºæœ¬ä¿¡æ¯
    user_id: str
    created_at: str
    updated_at: str

    # æƒ…ç»ªå¿«ç…§å†å²
    snapshots: List[EmotionSnapshot] = field(default_factory=list)

    # æ ¸å¿ƒåˆ†æ
    emotion_baseline: Dict[str, float] = field(default_factory=dict)
    emotion_trend: EmotionTrend = None
    emotion_cycles: List[EmotionCycle] = field(default_factory=list)
    emotion_clusters: List[EmotionCluster] = field(default_factory=list)

    # æ€§æ ¼ç”»åƒ
    personality: PersonalityProfile = field(default_factory=PersonalityProfile)

    # é£é™©è¯„ä¼°
    risk_prediction: RiskPrediction = None

    # æ·±åº¦æ´å¯Ÿ
    triggers: Dict[str, float] = field(default_factory=dict)  # è§¦å‘å› ç´ åŠå¼ºåº¦
    coping_strategies: Dict[str, float] = field(default_factory=dict)  # åº”å¯¹ç­–ç•¥åŠæ•ˆæœ
    emotion_network: Dict[str, Dict[str, float]] = field(default_factory=dict)  # æƒ…ç»ªå…³è”ç½‘ç»œ

    # ç»Ÿè®¡ä¿¡æ¯
    total_interactions: int = 0
    crisis_count: int = 0
    intervention_count: int = 0
    avg_recovery_time: float = 0.0  # å¹³å‡æ¢å¤æ—¶é—´ï¼ˆå°æ—¶ï¼‰

    # å…ƒæ•°æ®
    data_quality_score: float = 1.0  # æ•°æ®è´¨é‡åˆ†æ•°
    last_analysis_date: str = ""


class AdvancedEmotionProfileManager:
    """é«˜çº§æƒ…ç»ªç”»åƒç®¡ç†å™¨"""

    def __init__(self, config: Dict, storage_dir: str = "profiles"):
        self.config = config
        self.profile_config = config.get('emotion_profile', {})
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(exist_ok=True)

        # é…ç½®å‚æ•°
        self.baseline_window = self.profile_config.get('baseline_window', 30)
        self.trend_window = self.profile_config.get('trend_window', 7)  # è¶‹åŠ¿åˆ†æçª—å£ï¼ˆå¤©ï¼‰
        self.min_snapshots_for_analysis = self.profile_config.get('min_snapshots', 5)

        # DBTæƒ…ç»ªåˆ—è¡¨
        self.dbt_emotions = config.get('dbt_emotions', [])

        logger.info("é«˜çº§æƒ…ç»ªç”»åƒç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def create_profile(self, user_id: str) -> AdvancedEmotionProfile:
        """åˆ›å»ºæ–°ç”¨æˆ·ç”»åƒ"""
        now = datetime.now().isoformat()

        profile = AdvancedEmotionProfile(
            user_id=user_id,
            created_at=now,
            updated_at=now,
            emotion_trend=EmotionTrend(
                direction="stable",
                slope=0.0,
                confidence=0.0,
                timespan_days=0.0,
                change_percentage={}
            ),
            risk_prediction=RiskPrediction(
                next_crisis_probability=0.0,
                high_risk_time_windows=[],
                early_warning_signals=[],
                protective_factors=[],
                recommended_monitoring=[]
            )
        )

        self._save_profile(profile)
        logger.info(f"åˆ›å»ºé«˜çº§ç”¨æˆ·ç”»åƒ: {user_id}")
        return profile

    def load_profile(self, user_id: str) -> Optional[AdvancedEmotionProfile]:
        """åŠ è½½ç”¨æˆ·ç”»åƒ"""
        profile_path = self.storage_dir / f"{user_id}.json"

        if not profile_path.exists():
            return None

        try:
            with open(profile_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # é‡å»ºåµŒå¥—å¯¹è±¡
            snapshots = [EmotionSnapshot(**s) for s in data.get('snapshots', [])]

            trend_data = data.get('emotion_trend', {})
            emotion_trend = EmotionTrend(**trend_data) if trend_data else None

            cycles_data = data.get('emotion_cycles', [])
            emotion_cycles = [EmotionCycle(**c) for c in cycles_data]

            clusters_data = data.get('emotion_clusters', [])
            emotion_clusters = [EmotionCluster(**c) for c in clusters_data]

            personality_data = data.get('personality', {})
            personality = PersonalityProfile(**personality_data) if personality_data else PersonalityProfile()

            risk_data = data.get('risk_prediction', {})
            risk_prediction = RiskPrediction(**risk_data) if risk_data else None

            profile = AdvancedEmotionProfile(
                user_id=data['user_id'],
                created_at=data['created_at'],
                updated_at=data['updated_at'],
                snapshots=snapshots,
                emotion_baseline=data.get('emotion_baseline', {}),
                emotion_trend=emotion_trend,
                emotion_cycles=emotion_cycles,
                emotion_clusters=emotion_clusters,
                personality=personality,
                risk_prediction=risk_prediction,
                triggers=data.get('triggers', {}),
                coping_strategies=data.get('coping_strategies', {}),
                emotion_network=data.get('emotion_network', {}),
                total_interactions=data.get('total_interactions', 0),
                crisis_count=data.get('crisis_count', 0),
                intervention_count=data.get('intervention_count', 0),
                avg_recovery_time=data.get('avg_recovery_time', 0.0),
                data_quality_score=data.get('data_quality_score', 1.0),
                last_analysis_date=data.get('last_analysis_date', '')
            )

            logger.info(f"åŠ è½½é«˜çº§ç”¨æˆ·ç”»åƒ: {user_id}")
            return profile

        except Exception as e:
            logger.error(f"åŠ è½½ç”»åƒå¤±è´¥: {e}")
            return None

    def update_profile(self, profile: AdvancedEmotionProfile,
                      emotion_snapshot: EmotionSnapshot,
                      route_level: str,
                      risk_level: str):
        """
        æ·±åº¦æ›´æ–°ç”¨æˆ·ç”»åƒ
        åŒ…æ‹¬ï¼šè¶‹åŠ¿åˆ†æã€å‘¨æœŸæ£€æµ‹ã€èšç±»æŒ–æ˜ã€æ€§æ ¼æ¨æ–­ã€é£é™©é¢„æµ‹
        """
        # 1. æ·»åŠ å¿«ç…§å¹¶å¢å¼ºå…ƒæ•°æ®
        self._enrich_snapshot(emotion_snapshot)
        profile.snapshots.append(emotion_snapshot)
        profile.total_interactions += 1

        # 2. æ›´æ–°ç»Ÿè®¡
        if route_level == "L3_CRISIS":
            profile.crisis_count += 1
        if route_level in ["L2_INTERVENTION", "L3_CRISIS"]:
            profile.intervention_count += 1

        # 3. è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°
        profile.data_quality_score = self._calculate_data_quality(profile)

        # 4. æ·±åº¦åˆ†æï¼ˆéœ€è¦è¶³å¤Ÿæ•°æ®ï¼‰
        if len(profile.snapshots) >= self.min_snapshots_for_analysis:
            # 4.1 æ›´æ–°æƒ…ç»ªåŸºå‡†çº¿
            profile.emotion_baseline = self._calculate_advanced_baseline(profile.snapshots)

            # 4.2 åˆ†ææƒ…ç»ªè¶‹åŠ¿
            profile.emotion_trend = self._analyze_trend(profile.snapshots)

            # 4.3 æ£€æµ‹æƒ…ç»ªå‘¨æœŸ
            profile.emotion_cycles = self._detect_cycles(profile.snapshots)

            # 4.4 æŒ–æ˜æƒ…ç»ªèšç±»
            profile.emotion_clusters = self._mine_emotion_clusters(profile.snapshots)

            # 4.5 æ„å»ºæƒ…ç»ªå…³è”ç½‘ç»œ
            profile.emotion_network = self._build_emotion_network(profile.snapshots)

            # 4.6 æŒ–æ˜è§¦å‘å› ç´ 
            profile.triggers = self._mine_triggers(profile.snapshots)

            # 4.7 è¯„ä¼°åº”å¯¹ç­–ç•¥
            profile.coping_strategies = self._evaluate_coping_strategies(profile.snapshots)

            # 4.8 æ¨æ–­æ€§æ ¼ç‰¹å¾
            profile.personality = self._infer_personality(profile)

            # 4.9 é£é™©é¢„æµ‹
            profile.risk_prediction = self._predict_risk(profile)

            # 4.10 è®¡ç®—å¹³å‡æ¢å¤æ—¶é—´
            profile.avg_recovery_time = self._calculate_recovery_time(profile.snapshots)

        # 5. æ›´æ–°æ—¶é—´æˆ³
        profile.updated_at = datetime.now().isoformat()
        profile.last_analysis_date = datetime.now().isoformat()

        # 6. ä¿å­˜
        self._save_profile(profile)

        logger.info(f"æ·±åº¦æ›´æ–°ç”¨æˆ·ç”»åƒ: {profile.user_id}")

    def _enrich_snapshot(self, snapshot: EmotionSnapshot):
        """å¢å¼ºå¿«ç…§å…ƒæ•°æ®"""
        dt = datetime.fromtimestamp(snapshot.timestamp)

        # ä¸€å¤©ä¸­çš„æ—¶æ®µ
        hour = dt.hour
        if 5 <= hour < 12:
            snapshot.time_of_day = "morning"
        elif 12 <= hour < 18:
            snapshot.time_of_day = "afternoon"
        elif 18 <= hour < 22:
            snapshot.time_of_day = "evening"
        else:
            snapshot.time_of_day = "night"

        # æ˜ŸæœŸå‡ 
        snapshot.day_of_week = dt.strftime("%A")
        snapshot.is_weekend = dt.weekday() >= 5

    def _calculate_data_quality(self, profile: AdvancedEmotionProfile) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
        if not profile.snapshots:
            return 0.0

        score = 1.0

        # 1. æ•°æ®é‡ï¼ˆè¶Šå¤šè¶Šå¥½ï¼Œä¸Šé™100æ¬¡ï¼‰
        data_count = len(profile.snapshots)
        score *= min(1.0, data_count / 50.0)

        # 2. æ•°æ®è¦†ç›–åº¦ï¼ˆæœ€è¿‘30å¤©çš„æ•°æ®ï¼‰
        now = datetime.now().timestamp()
        recent_30_days = [s for s in profile.snapshots if now - s.timestamp <= 30 * 24 * 3600]
        score *= min(1.0, len(recent_30_days) / 10.0)

        # 3. å¤šæ¨¡æ€æ•°æ®å æ¯”
        multimodal_count = sum(1 for s in profile.snapshots if s.input_type in ['audio', 'image'])
        if data_count > 0:
            score *= (0.7 + 0.3 * (multimodal_count / data_count))

        return min(1.0, max(0.0, score))

    def _calculate_advanced_baseline(self, snapshots: List[EmotionSnapshot]) -> Dict[str, float]:
        """
        è®¡ç®—é«˜çº§æƒ…ç»ªåŸºå‡†çº¿
        ä½¿ç”¨åŠ æƒå¹³å‡ï¼ˆè¿‘æœŸæƒé‡æ›´é«˜ï¼‰+ å¼‚å¸¸å€¼è¿‡æ»¤
        """
        if not snapshots:
            return {}

        # ç­›é€‰æœ€è¿‘30å¤©çš„å¿«ç…§
        now = datetime.now().timestamp()
        cutoff_time = now - (self.baseline_window * 24 * 3600)
        recent_snapshots = [s for s in snapshots if s.timestamp >= cutoff_time]

        if not recent_snapshots:
            recent_snapshots = snapshots

        # æ—¶é—´åŠ æƒï¼ˆè¶Šè¿‘æƒé‡è¶Šé«˜ï¼‰
        timestamps = [s.timestamp for s in recent_snapshots]
        min_ts = min(timestamps)
        max_ts = max(timestamps)
        time_span = max_ts - min_ts + 1

        weights = [(s.timestamp - min_ts) / time_span for s in recent_snapshots]

        # è®¡ç®—åŠ æƒå¹³å‡
        baseline = {}
        emotion_names = set()
        for s in recent_snapshots:
            emotion_names.update(s.emotions.keys())

        for emotion in emotion_names:
            scores = [s.emotions.get(emotion, 0.0) for s in recent_snapshots]

            # å»é™¤å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨IQRæ–¹æ³•ï¼‰
            if len(scores) >= 4:
                q1, q3 = np.percentile(scores, [25, 75])
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr

                filtered_scores = [s for s in scores if lower_bound <= s <= upper_bound]
                filtered_weights = [w for s, w in zip(scores, weights) if lower_bound <= s <= upper_bound]
            else:
                filtered_scores = scores
                filtered_weights = weights

            if sum(filtered_weights) > 0:
                baseline[emotion] = float(np.average(filtered_scores, weights=filtered_weights))
            else:
                baseline[emotion] = 0.0

        return baseline

    def _analyze_trend(self, snapshots: List[EmotionSnapshot]) -> EmotionTrend:
        """
        åˆ†ææƒ…ç»ªè¶‹åŠ¿
        æ£€æµ‹ä¸Šå‡ã€ä¸‹é™ã€ç¨³å®šã€æ³¢åŠ¨
        """
        if len(snapshots) < 3:
            return EmotionTrend(
                direction="stable",
                slope=0.0,
                confidence=0.0,
                timespan_days=0.0,
                change_percentage={}
            )

        # åªåˆ†ææœ€è¿‘Nå¤©çš„æ•°æ®
        now = datetime.now().timestamp()
        cutoff_time = now - (self.trend_window * 24 * 3600)
        recent_snapshots = sorted([s for s in snapshots if s.timestamp >= cutoff_time], key=lambda x: x.timestamp)

        if len(recent_snapshots) < 3:
            recent_snapshots = sorted(snapshots, key=lambda x: x.timestamp)[-10:]

        # è®¡ç®—æ—¶é—´è·¨åº¦
        timespan_days = (recent_snapshots[-1].timestamp - recent_snapshots[0].timestamp) / (24 * 3600)

        # æå–æ€»ä½“æƒ…ç»ªå¼ºåº¦ï¼ˆæ‰€æœ‰æƒ…ç»ªçš„å¹³å‡ï¼‰
        overall_scores = []
        timestamps = []
        for s in recent_snapshots:
            overall = max(s.emotions.values()) if s.emotions else 0.0
            overall_scores.append(overall)
            timestamps.append(s.timestamp)

        # çº¿æ€§å›å½’è®¡ç®—è¶‹åŠ¿
        if len(timestamps) >= 2:
            timestamps_norm = [(t - timestamps[0]) / (timestamps[-1] - timestamps[0] + 1) for t in timestamps]
            slope, intercept, r_value, p_value, std_err = stats.linregress(timestamps_norm, overall_scores)

            # åˆ¤æ–­è¶‹åŠ¿æ–¹å‘
            if abs(slope) < 0.05:
                direction = "stable"
            elif slope > 0:
                direction = "rising"
            else:
                direction = "falling"

            # æ£€æµ‹æ³¢åŠ¨æ€§
            if len(overall_scores) >= 5:
                volatility = np.std(overall_scores)
                if volatility > 0.3:
                    direction = "volatile"

            confidence = abs(r_value)

            # è®¡ç®—å„æƒ…ç»ªå˜åŒ–ç™¾åˆ†æ¯”
            change_percentage = {}
            first_emotions = recent_snapshots[0].emotions
            last_emotions = recent_snapshots[-1].emotions

            for emotion in set(list(first_emotions.keys()) + list(last_emotions.keys())):
                first_score = first_emotions.get(emotion, 0.0)
                last_score = last_emotions.get(emotion, 0.0)
                if first_score > 0:
                    change_percentage[emotion] = ((last_score - first_score) / first_score) * 100
                else:
                    change_percentage[emotion] = 0.0

            return EmotionTrend(
                direction=direction,
                slope=float(slope),
                confidence=float(confidence),
                timespan_days=float(timespan_days),
                change_percentage=change_percentage
            )

        return EmotionTrend(
            direction="stable",
            slope=0.0,
            confidence=0.0,
            timespan_days=float(timespan_days),
            change_percentage={}
        )

    def _detect_cycles(self, snapshots: List[EmotionSnapshot]) -> List[EmotionCycle]:
        """
        æ£€æµ‹æƒ…ç»ªå‘¨æœŸ
        åŒ…æ‹¬ï¼šæ—¥å†…å‘¨æœŸã€å‘¨å†…å‘¨æœŸã€æœˆåº¦å‘¨æœŸ
        """
        if len(snapshots) < 10:
            return []

        cycles = []
        now = datetime.now().timestamp()

        # 1. æ—¥å†…å‘¨æœŸåˆ†æ
        daily_patterns = defaultdict(list)
        for s in snapshots:
            if now - s.timestamp <= 30 * 24 * 3600:  # æœ€è¿‘30å¤©
                daily_patterns[s.time_of_day].append(max(s.emotions.values(), default=0.0))

        if len(daily_patterns) >= 3:
            times = list(daily_patterns.keys())
            avg_scores = [np.mean(daily_patterns[t]) for t in times]

            peak_idx = np.argmax(avg_scores)
            low_idx = np.argmin(avg_scores)

            cycles.append(EmotionCycle(
                cycle_type="daily",
                peak_times=[times[peak_idx]],
                low_times=[times[low_idx]],
                strength=float(np.std(avg_scores)),
                pattern={"daily": avg_scores}
            ))

        # 2. å‘¨å†…å‘¨æœŸåˆ†æ
        weekday_patterns = defaultdict(list)
        for s in snapshots:
            if now - s.timestamp <= 60 * 24 * 3600:  # æœ€è¿‘60å¤©
                weekday_patterns[s.day_of_week].append(max(s.emotions.values(), default=0.0))

        if len(weekday_patterns) >= 5:
            weekdays = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]
            avg_scores = []
            for wd in weekdays:
                scores = weekday_patterns.get(wd, [])
                avg_scores.append(np.mean(scores) if scores else 0.0)

            peak_idx = np.argmax(avg_scores)
            low_idx = np.argmin(avg_scores)

            cycles.append(EmotionCycle(
                cycle_type="weekly",
                peak_times=[weekdays[peak_idx]],
                low_times=[weekdays[low_idx]],
                strength=float(np.std(avg_scores)),
                pattern={"weekly": avg_scores}
            ))

        # 3. å·¥ä½œæ—¥ vs å‘¨æœ«
        weekday_scores = [max(s.emotions.values(), default=0.0) for s in snapshots if not s.is_weekend]
        weekend_scores = [max(s.emotions.values(), default=0.0) for s in snapshots if s.is_weekend]

        if weekday_scores and weekend_scores:
            cycles.append(EmotionCycle(
                cycle_type="weekend_vs_weekday",
                peak_times=["weekday" if np.mean(weekday_scores) > np.mean(weekend_scores) else "weekend"],
                low_times=["weekend" if np.mean(weekday_scores) < np.mean(weekend_scores) else "weekday"],
                strength=float(abs(np.mean(weekday_scores) - np.mean(weekend_scores))),
                pattern={"weekday": [np.mean(weekday_scores)], "weekend": [np.mean(weekend_scores)]}
            ))

        return cycles

    def _mine_emotion_clusters(self, snapshots: List[EmotionSnapshot]) -> List[EmotionCluster]:
        """
        æŒ–æ˜æƒ…ç»ªèšç±»æ¨¡å¼
        å‘ç°ç»å¸¸ä¸€èµ·å‡ºç°çš„æƒ…ç»ªç»„åˆ
        """
        if len(snapshots) < 5:
            return []

        # 1. æå–æ‰€æœ‰éé›¶æƒ…ç»ªå‘é‡
        emotion_vectors = []
        for s in snapshots:
            vector = {k: v for k, v in s.emotions.items() if v > 0.1}
            if vector:
                emotion_vectors.append((vector, s.context, s.route_level))

        if not emotion_vectors:
            return []

        # 2. ç®€å•èšç±»ï¼ˆåŸºäºç›¸ä¼¼åº¦ï¼‰
        clusters = []
        used_indices = set()

        for i, (vector, context, route) in enumerate(emotion_vectors):
            if i in used_indices:
                continue

            # æ‰¾åˆ°ç›¸ä¼¼çš„å‘é‡
            similar_vectors = []
            for j, (v2, c2, r2) in enumerate(emotion_vectors):
                if j <= i or j in used_indices:
                    continue

                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                similarity = self._cosine_similarity(vector, v2)
                if similarity > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    similar_vectors.append((v2, c2, r2))
                    used_indices.add(j)

            if similar_vectors:
                # åˆå¹¶æƒ…ç»ªå‘é‡
                all_vectors = [vector] + [v for v, _, _ in similar_vectors]
                avg_emotions = {}
                for emo in set().union(*[set(v.keys()) for v in all_vectors]):
                    avg_emotions[emo] = float(np.mean([v.get(emo, 0.0) for v in all_vectors]))

                # æå–è§¦å‘å› ç´ 
                triggers = list(set([context for _, context, _ in similar_vectors] + [context]))

                # æå–ç»“æœ
                outcomes = list(set([route for _, _, route in similar_vectors] + [route]))

                # ç”Ÿæˆèšç±»åç§°
                top_emotions = sorted(avg_emotions.items(), key=lambda x: -x[1])[:3]
                cluster_name = "_".join([e for e, _ in top_emotions])

                clusters.append(EmotionCluster(
                    cluster_name=cluster_name,
                    emotions=avg_emotions,
                    frequency=len(all_vectors),
                    triggers=triggers[:5],
                    outcomes=outcomes
                ))

                used_indices.add(i)

        # 3. æŒ‰é¢‘ç‡æ’åº
        clusters.sort(key=lambda x: -x.frequency)

        return clusters[:10]  # è¿”å›å‰10ä¸ªèšç±»

    def _cosine_similarity(self, vec1: Dict, vec2: Dict) -> float:
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        all_keys = set(vec1.keys()) | set(vec2.keys())

        dot_product = sum(vec1.get(k, 0) * vec2.get(k, 0) for k in all_keys)
        norm1 = np.sqrt(sum(v ** 2 for v in vec1.values()))
        norm2 = np.sqrt(sum(v ** 2 for v in vec2.values()))

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    def _build_emotion_network(self, snapshots: List[EmotionSnapshot]) -> Dict[str, Dict[str, float]]:
        """
        æ„å»ºæƒ…ç»ªå…³è”ç½‘ç»œ
        å‘ç°å“ªäº›æƒ…ç»ªç»å¸¸ä¸€èµ·å‡ºç°
        """
        network = {}

        if len(snapshots) < 3:
            return network

        # è®¡ç®—å…±ç°çŸ©é˜µ
        co_occurrence = defaultdict(lambda: defaultdict(int))

        for s in snapshots:
            emotions = [k for k, v in s.emotions.items() if v > 0.1]
            for e1 in emotions:
                for e2 in emotions:
                    if e1 != e2:
                        co_occurrence[e1][e2] += 1

        # å½’ä¸€åŒ–å¹¶è½¬æ¢ä¸ºå…³è”å¼ºåº¦
        for e1, related in co_occurrence.items():
            total = sum(related.values())
            network[e1] = {e2: count / total for e2, count in related.items()}

        # åªä¿ç•™å¼ºå…³è”ï¼ˆ>0.3ï¼‰
        network = {
            e1: {e2: s for e2, s in related.items() if s > 0.3}
            for e1, related in network.items()
        }

        return network

    def _mine_triggers(self, snapshots: List[EmotionSnapshot]) -> Dict[str, float]:
        """
        æ·±åº¦æŒ–æ˜è§¦å‘å› ç´ 
        åŸºäºä¸Šä¸‹æ–‡å’Œæƒ…ç»ªå¼ºåº¦
        """
        triggers = {}

        if not snapshots:
            return triggers

        # 1. ä»contextä¸­æå–å…³é”®è¯
        context_intensity = defaultdict(list)
        for s in snapshots:
            if s.context:
                intensity = max(s.emotions.values(), default=0.0)
                if intensity > 0.3:  # åªè€ƒè™‘é«˜æƒ…ç»ªå¼ºåº¦çš„context
                    context_intensity[s.context].append(intensity)

        # 2. è®¡ç®—å¹³å‡è§¦å‘å¼ºåº¦
        for context, intensities in context_intensity.items():
            if len(intensities) >= 2:
                triggers[context] = float(np.mean(intensities))

        # 3. æ’åºå¹¶è¿”å›å‰10ä¸ª
        triggers = dict(sorted(triggers.items(), key=lambda x: -x[1])[:10])

        return triggers

    def _evaluate_coping_strategies(self, snapshots: List[EmotionSnapshot]) -> Dict[str, float]:
        """
        è¯„ä¼°åº”å¯¹ç­–ç•¥æ•ˆæœ
        é€šè¿‡è§‚å¯Ÿæƒ…ç»ªæ¢å¤æ¨¡å¼
        """
        strategies = {}

        if len(snapshots) < 5:
            return strategies

        # æ‰¾å‡ºå±æœºåçš„æ¢å¤åºåˆ—
        for i in range(len(snapshots) - 1):
            current = snapshots[i]
            next_s = snapshots[i + 1]

            # å½“å‰æ˜¯é«˜è´Ÿé¢æƒ…ç»ªï¼Œä¸‹æ¬¡ä½äº† â†’ æœ‰æ•ˆç­–ç•¥
            current_negative = sum(current.emotions.get(e, 0) for e in
                                ['æ‚²ä¼¤', 'ç„¦è™‘', 'ç»æœ›', 'ç©ºè™šæ„Ÿ', 'è‡ªä¼¤å†²åŠ¨'])
            next_negative = sum(next_s.emotions.get(e, 0) for e in
                             ['æ‚²ä¼¤', 'ç„¦è™‘', 'ç»æœ›', 'ç©ºè™šæ„Ÿ', 'è‡ªä¼¤å†²åŠ¨'])

            if current_negative > 0.5 and next_negative < current_negative * 0.5:
                # æœ‰æ•ˆçš„æ¢å¤
                if current.context:
                    effectiveness = (current_negative - next_negative) / current_negative
                    strategies[current.context] = max(
                        strategies.get(current.context, 0.0),
                        effectiveness
                    )

        return strategies

    def _infer_personality(self, profile: AdvancedEmotionProfile) -> PersonalityProfile:
        """
        åŸºäºçœŸå®æ•°æ®æ¨æ–­æ€§æ ¼ç‰¹å¾
        """
        snapshots = profile.snapshots
        personality = PersonalityProfile()

        if len(snapshots) < 5:
            return personality

        # 1. ç¥ç»è´¨ - åŸºäºæƒ…ç»ªç¨³å®šæ€§
        if profile.emotion_trend:
            stability = 1.0 - abs(profile.emotion_trend.slope)
            personality.neuroticism = 1.0 - stability

        # 2. å¤–å‘æ€§ - åŸºäºæ—¶é—´æ¨¡å¼ï¼ˆå‘¨æœ«vså·¥ä½œæ—¥ï¼‰
        weekend_scores = [max([0] + list(s.emotions.values())) for s in snapshots if s.is_weekend]
        weekday_scores = [max([0] + list(s.emotions.values())) for s in snapshots if not s.is_weekend]

        if weekend_scores and weekday_scores:
            if np.mean(weekend_scores) > np.mean(weekday_scores):
                personality.extraversion = 0.7  # å‘¨æœ«æ›´æ´»è·ƒ
            else:
                personality.extraversion = 0.3  # å·¥ä½œæ—¥æ›´æ´»è·ƒ

        # 3. å°½è´£æ€§ - åŸºäºè§„å¾‹æ€§
        if len(snapshots) >= 10:
            # è®¡ç®—äº’åŠ¨æ—¶é—´é—´éš”çš„æ ‡å‡†å·®
            intervals = []
            for i in range(1, len(snapshots)):
                interval = snapshots[i].timestamp - snapshots[i-1].timestamp
                intervals.append(interval)

            if intervals:
                regularity = 1.0 / (1.0 + np.std(intervals) / (np.mean(intervals) + 1))
                personality.conscientiousness = regularity

        # 4. å®œäººæ€§ - åŸºäºæƒ…ç»ªç±»å‹
        anger_scores = [s.emotions.get('æ„¤æ€’', 0) for s in snapshots]
        empathy_scores = [s.emotions.get('å†…ç–š', 0) + s.emotions.get('ç¾æ„§', 0) for s in snapshots]

        if anger_scores and empathy_scores:
            avg_anger = np.mean(anger_scores)
            avg_empathy = np.mean(empathy_scores)
            personality.agreeableness = 1.0 - avg_anger + avg_empathy * 0.5
            personality.agreeableness = max(0.0, min(1.0, personality.agreeableness))

        # 5. å¼€æ”¾æ€§ - åŸºäºæƒ…ç»ªå¤šæ ·æ€§
        emotion_diversity = len(set().union(*[set(s.emotions.keys()) for s in snapshots]))
        personality.openness = min(1.0, emotion_diversity / len(self.dbt_emotions))

        # 6. å†³ç­–é£æ ¼ - åŸºäºå˜åŒ–æ¨¡å¼
        if profile.emotion_clusters:
            cluster_count = len(profile.emotion_clusters)
            if cluster_count >= 5:
                personality.decision_style = "flexible"
            elif cluster_count <= 2:
                personality.decision_style = "rigid"
            else:
                personality.decision_style = "analytical"

        # 7. å‹åŠ›åº”å¯¹ - åŸºäºå±æœºå†å²
        if profile.total_interactions > 0:
            crisis_ratio = profile.crisis_count / profile.total_interactions
            if crisis_ratio > 0.3:
                personality.stress_response = "withdrawn"
            elif crisis_ratio < 0.1:
                personality.stress_response = "active"

        # 8. æ¢å¤åŠ› - åŸºäºæ¢å¤æ—¶é—´
        if profile.avg_recovery_time > 0:
            personality.resilience_score = 1.0 / (1.0 + profile.avg_recovery_time / 24)  # æŒ‰å¤©å½’ä¸€åŒ–

        # 9. æƒ…ç»ªè§‰å¯Ÿ - åŸºäºè¡¨è¾¾ç»†è…»åº¦
        avg_emotions_per_snapshot = np.mean([len([e for e, v in s.emotions.items() if v > 0.1])
                                              for s in snapshots])
        personality.emotional_awareness = min(1.0, avg_emotions_per_snapshot / 5.0)

        # 10. å†²åŠ¨æ§åˆ¶ - åŸºäºæ¿€è¶Šå’Œè‡ªä¼¤å†²åŠ¨
        impulsiveness_scores = [s.emotions.get('æ¿€è¶Š', 0) + s.emotions.get('è‡ªä¼¤å†²åŠ¨', 0) * 2
                              for s in snapshots]
        if impulsiveness_scores:
            avg_impulsiveness = np.mean(impulsiveness_scores)
            personality.impulse_control = 1.0 - min(1.0, avg_impulsiveness)

        # å½’ä¸€åŒ–åˆ°[0,1]
        for field_name, field_type in personality.__dataclass_fields__.items():
            if field_name != 'coping_style':
                value = getattr(personality, field_name)
                if isinstance(value, float):
                    setattr(personality, field_name, max(0.0, min(1.0, value)))

        return personality

    def _predict_risk(self, profile: AdvancedEmotionProfile) -> RiskPrediction:
        """
        é£é™©é¢„æµ‹
        é¢„æµ‹ä¸‹æ¬¡å±æœºæ¦‚ç‡å’Œé«˜é£é™©æ—¶æ®µ
        """
        prediction = RiskPrediction(
            next_crisis_probability=0.0,
            high_risk_time_windows=[],
            early_warning_signals=[],
            protective_factors=[],
            recommended_monitoring=[]
        )

        if len(profile.snapshots) < 5:
            return prediction

        # 1. è®¡ç®—å±æœºæ¦‚ç‡ï¼ˆåŸºäºå†å²é¢‘ç‡ï¼‰
        if profile.total_interactions > 0:
            crisis_ratio = profile.crisis_count / profile.total_interactions
            prediction.next_crisis_probability = crisis_ratio

        # 2. è¯†åˆ«é«˜é£é™©æ—¶æ®µ
        if profile.emotion_cycles:
            for cycle in profile.emotion_cycles:
                if cycle.strength > 0.2:  # æ˜æ˜¾çš„å‘¨æœŸ
                    prediction.high_risk_time_windows.append(f"{cycle.cycle_type}_{cycle.peak_times[0]}")

        # 3. æ—©æœŸé¢„è­¦ä¿¡å·
        # æ£€æŸ¥æœ€è¿‘çš„è¶‹åŠ¿
        if profile.emotion_trend and profile.emotion_trend.direction == "rising":
            if profile.emotion_trend.slope > 0.1:
                prediction.early_warning_signals.append("æƒ…ç»ªæŒç»­ä¸Šå‡")

        # æ£€æŸ¥é«˜é¢‘ç‡çš„è´Ÿé¢æƒ…ç»ª
        recent_negative_avg = np.mean([
            sum(s.emotions.get(e, 0) for e in ['æ‚²ä¼¤', 'ç„¦è™‘', 'ç»æœ›'])
            for s in profile.snapshots[-10:]
        ])
        if recent_negative_avg > 0.4:
            prediction.early_warning_signals.append("è´Ÿé¢æƒ…ç»ªæŒç»­é«˜ä½")

        # æ£€æŸ¥æ³¢åŠ¨æ€§
        if profile.emotion_trend and profile.emotion_trend.direction == "volatile":
            prediction.early_warning_signals.append("æƒ…ç»ªæåº¦ä¸ç¨³å®š")

        # 4. ä¿æŠ¤å› ç´ 
        if profile.coping_strategies:
            prediction.protective_factors = list(profile.coping_strategies.keys())[:5]

        if profile.personality.resilience_score > 0.7:
            prediction.protective_factors.append("é«˜æ¢å¤åŠ›")

        if profile.intervention_count > 0 and profile.crisis_count / (profile.intervention_count + 1) < 0.5:
            prediction.protective_factors.append("å¹²é¢„å“åº”è‰¯å¥½")

        # 5. å»ºè®®ç›‘æ§çš„æŒ‡æ ‡
        prediction.recommended_monitoring = [
            "æ•´ä½“æƒ…ç»ªå¼ºåº¦",
            "å±æœºè§¦å‘é¢‘ç‡",
            "æ¢å¤æ—¶é—´é•¿åº¦"
        ]

        if profile.emotion_trend:
            prediction.recommended_monitoring.append(f"è¶‹åŠ¿æ–¹å‘: {profile.emotion_trend.direction}")

        return prediction

    def _calculate_recovery_time(self, snapshots: List[EmotionSnapshot]) -> float:
        """è®¡ç®—å¹³å‡æ¢å¤æ—¶é—´ï¼ˆå°æ—¶ï¼‰"""
        recovery_times = []

        for i in range(len(snapshots) - 1):
            current = snapshots[i]
            next_s = snapshots[i + 1]

            # å½“å‰æ˜¯å±æœºï¼Œä¸‹æ¬¡ä¸æ˜¯
            if current.route_level == "L3_CRISIS" and next_s.route_level != "L3_CRISIS":
                recovery_hours = (next_s.timestamp - current.timestamp) / 3600
                recovery_times.append(recovery_hours)

        if recovery_times:
            return float(np.mean(recovery_times))

        return 0.0

    def generate_profile_report(self, profile: AdvancedEmotionProfile) -> str:
        """ç”Ÿæˆè¯¦ç»†çš„ç”»åƒæŠ¥å‘Š"""
        lines = []
        lines.append("=" * 70)
        lines.append(f"é«˜çº§ç”¨æˆ·æƒ…ç»ªç”»åƒæŠ¥å‘Š")
        lines.append(f"ç”¨æˆ·ID: {profile.user_id}")
        lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append(f"æ•°æ®è´¨é‡åˆ†æ•°: {profile.data_quality_score:.2%}")
        lines.append("=" * 70)
        lines.append("")

        # 1. åŸºæœ¬ä¿¡æ¯
        lines.append("## ğŸ“Š åŸºæœ¬ä¿¡æ¯")
        lines.append(f"- æ€»äº’åŠ¨æ¬¡æ•°: {profile.total_interactions}")
        lines.append(f"- å±æœºæ¬¡æ•°: {profile.crisis_count}")
        lines.append(f"- å¹²é¢„æ¬¡æ•°: {profile.intervention_count}")
        lines.append(f"- å¹³å‡æ¢å¤æ—¶é—´: {profile.avg_recovery_time:.1f} å°æ—¶")
        lines.append("")

        # 2. æƒ…ç»ªåŸºå‡†çº¿
        lines.append("## ğŸ¯ æƒ…ç»ªåŸºå‡†çº¿")
        if profile.emotion_baseline:
            sorted_emotions = sorted(profile.emotion_baseline.items(), key=lambda x: -x[1])
            for emotion, score in sorted_emotions[:10]:
                if score > 0.05:
                    bar = "â–ˆ" * int(score * 20)
                    lines.append(f"- {emotion}: {score:.2%} {bar}")
        else:
            lines.append("æš‚æ— æ•°æ®")
        lines.append("")

        # 3. æƒ…ç»ªè¶‹åŠ¿
        lines.append("## ğŸ“ˆ æƒ…ç»ªè¶‹åŠ¿")
        if profile.emotion_trend and profile.emotion_trend.confidence > 0.3:
            trend = profile.emotion_trend
            direction_icon = {"rising": "ğŸ“ˆ", "falling": "ğŸ“‰", "stable": "â¡ï¸", "volatile": "ğŸ“Š"}
            lines.append(f"- è¶‹åŠ¿æ–¹å‘: {direction_icon.get(trend.direction, '?')} {trend.direction.upper()}")
            lines.append(f"- å˜åŒ–æ–œç‡: {trend.slope:.4f}")
            lines.append(f"- ç½®ä¿¡åº¦: {trend.confidence:.2%}")
            lines.append(f"- åˆ†æå‘¨æœŸ: {trend.timespan_days:.1f} å¤©")

            if trend.change_percentage:
                lines.append("- æƒ…ç»ªå˜åŒ–:")
                for emotion, change in sorted(trend.change_percentage.items(), key=lambda x: -abs(x[1]))[:5]:
                    icon = "ğŸ“ˆ" if change > 0 else "ğŸ“‰"
                    lines.append(f"  {icon} {emotion}: {change:+.1f}%")
        else:
            lines.append("æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿")
        lines.append("")

        # 4. æƒ…ç»ªå‘¨æœŸ
        lines.append("## ğŸ”„ æƒ…ç»ªå‘¨æœŸ")
        if profile.emotion_cycles:
            for cycle in profile.emotion_cycles[:3]:
                lines.append(f"- {cycle.cycle_type.upper()} å‘¨æœŸ:")
                lines.append(f"  é«˜å³°æ—¶æ®µ: {', '.join(cycle.peak_times)}")
                lines.append(f"  ä½è°·æ—¶æ®µ: {', '.join(cycle.low_times)}")
                lines.append(f"  å‘¨æœŸå¼ºåº¦: {cycle.strength:.2%}")
        else:
            lines.append("æš‚æ— æ˜æ˜¾å‘¨æœŸ")
        lines.append("")

        # 5. æƒ…ç»ªèšç±»
        lines.append("## ğŸ” æƒ…ç»ªæ¨¡å¼")
        if profile.emotion_clusters:
            for cluster in profile.emotion_clusters[:5]:
                lines.append(f"- {cluster.cluster_name} (å‡ºç°{cluster.frequency}æ¬¡)")
                top_emotions = sorted(cluster.emotions.items(), key=lambda x: -x[1])[:3]
                lines.append(f"  å…¸å‹æƒ…ç»ª: {', '.join([f'{e}:{s:.2f}' for e, s in top_emotions])}")
                if cluster.triggers:
                    lines.append(f"  è§¦å‘å› ç´ : {', '.join(cluster.triggers[:3])}")
        else:
            lines.append("æš‚æ— èšç±»æ•°æ®")
        lines.append("")

        # 6. æ€§æ ¼ç”»åƒ
        lines.append("## ğŸ‘¤ æ€§æ ¼ç”»åƒ")
        p = profile.personality
        lines.append(f"- å¼€æ”¾æ€§: {p.openness:.2%}")
        lines.append(f"- å°½è´£æ€§: {p.conscientiousness:.2%}")
        lines.append(f"- å¤–å‘æ€§: {p.extraversion:.2%}")
        lines.append(f"- å®œäººæ€§: {p.agreeableness:.2%}")
        lines.append(f"- ç¥ç»è´¨: {p.neuroticism:.2%}")
        lines.append(f"- å†³ç­–é£æ ¼: {p.decision_style}")
        lines.append(f"- å‹åŠ›åº”å¯¹: {p.stress_response}")
        lines.append(f"- æ¢å¤åŠ›: {p.resilience_score:.2%}")
        lines.append("")

        # 7. é£é™©é¢„æµ‹
        lines.append("## âš ï¸ é£é™©é¢„æµ‹")
        if profile.risk_prediction:
            rp = profile.risk_prediction
            lines.append(f"- ä¸‹æ¬¡å±æœºæ¦‚ç‡: {rp.next_crisis_probability:.2%}")
            if rp.high_risk_time_windows:
                lines.append(f"- é«˜é£é™©æ—¶æ®µ: {', '.join(rp.high_risk_time_windows)}")
            if rp.early_warning_signals:
                lines.append(f"- æ—©æœŸé¢„è­¦ä¿¡å·:")
                for signal in rp.early_warning_signals:
                    lines.append(f"  âš ï¸ {signal}")
            if rp.protective_factors:
                lines.append(f"- ä¿æŠ¤å› ç´ :")
                for factor in rp.protective_factors:
                    lines.append(f"  âœ… {factor}")
        lines.append("")

        # 8. åº”å¯¹ç­–ç•¥
        lines.append("## ğŸ’¡ æœ‰æ•ˆåº”å¯¹ç­–ç•¥")
        if profile.coping_strategies:
            for strategy, effectiveness in sorted(profile.coping_strategies.items(), key=lambda x: -x[1])[:5]:
                lines.append(f"- {strategy} (æ•ˆæœ: {effectiveness:.2%})")
        else:
            lines.append("æš‚æ— è¶³å¤Ÿæ•°æ®")
        lines.append("")

        lines.append("=" * 70)
        lines.append(f"æŠ¥å‘Šç”Ÿæˆäº: {datetime.now().isoformat()}")
        lines.append("=" * 70)

        return "\n".join(lines)

    def export_for_self_agent(self, profile: AdvancedEmotionProfile) -> Dict:
        """å¯¼å‡ºä¸ºSelf-Agentå¯ç”¨çš„æ ¼å¼"""
        return {
            'user_id': profile.user_id,
            'personality': asdict(profile.personality),
            'emotion_baseline': profile.emotion_baseline,
            'typical_patterns': [asdict(c) for c in profile.emotion_clusters[:5]],
            'triggers': profile.triggers,
            'coping_strategies': profile.coping_strategies,
            'risk_prediction': asdict(profile.risk_prediction) if profile.risk_prediction else {},
            'emotion_network': profile.emotion_network
        }

    def _save_profile(self, profile: AdvancedEmotionProfile):
        """ä¿å­˜ç”»åƒåˆ°æ–‡ä»¶"""
        profile_path = self.storage_dir / f"{profile.user_id}.json"

        data = {
            'user_id': profile.user_id,
            'created_at': profile.created_at,
            'updated_at': profile.updated_at,
            'snapshots': [asdict(s) for s in profile.snapshots],
            'emotion_baseline': profile.emotion_baseline,
            'emotion_trend': asdict(profile.emotion_trend) if profile.emotion_trend else None,
            'emotion_cycles': [asdict(c) for c in profile.emotion_cycles],
            'emotion_clusters': [asdict(c) for c in profile.emotion_clusters],
            'personality': asdict(profile.personality),
            'risk_prediction': asdict(profile.risk_prediction) if profile.risk_prediction else None,
            'triggers': profile.triggers,
            'coping_strategies': profile.coping_strategies,
            'emotion_network': profile.emotion_network,
            'total_interactions': profile.total_interactions,
            'crisis_count': profile.crisis_count,
            'intervention_count': profile.intervention_count,
            'avg_recovery_time': profile.avg_recovery_time,
            'data_quality_score': profile.data_quality_score,
            'last_analysis_date': profile.last_analysis_date
        }

        with open(profile_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.debug(f"ä¿å­˜é«˜çº§ç”»åƒ: {profile.user_id}")
